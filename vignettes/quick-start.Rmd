---
title: "Analyzing single-cell RNA-seq perturbation with DECAL"
author: "AndrÃ© M. Ribeiro-dos-Santos"
date: "`r format(Sys.Date(), '%m/%d/%Y')`"
abstract: >
  TODO: Package abstract
  decal package version: `r packageVersion("decal")`
output:
  rmarkdown::html_document:
    highlight: pygments
    toc: true
    df_print: kable
    fig_width: 6
    fig_caption: true
bibliography: decal.bib
vignette: >
  %\VignetteIndexEntry{Analyzing single-cell RNA-seq perturbation with scCloneDE}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  %\usepackage[utf8]{inputenc}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  tidy = FALSE, cache = FALSE, dev = "png", collapse = TRUE, comment = "#>",
  message = FALSE, error = FALSE, warning = TRUE
)
options(scipen = 1, digits = 2)
```

# Default Workflow

**DECAL** (**D**ifferential **E**xpression analysis of **C**lonal
**A**lterations **L**ocal effects) provide you tools to conduct differential
expression analysis of single-cell perturbations to potential interacting
genes.
Similar to other expression analysis tools, it models gene expression using
a _Negative Binomial_ (or _Gamma-Poisson_) regression, modeling each gene
UMI count by the cell total count and the cell alteration status.

## Features

- `decal` is compatible with **tidyverse** analysis.
- Includes a suit of simulation functions to generate your own dataset based
  `decal` model and evaluate your statistical power.
- Package has few dependencies, requiring only `MASS`, `fastglm` and `Matrix`.
- Can evaluate specific clonal alteration and gene effect, instead of
  modeling all genes and all alterations. This allow us to quickly investigate
  a large number of interactions skipping unlikely effects.

## Instalation

To install `decal` package current version, open your R terminal and type:

```{r installation, eval = FALSE}
## Install remotes if not available with
## install.install.packages("remotes")
remotes::install_github("mauranolab/decal")
```

## Quick Start

`decal` is the package main function which estimates each gene dispersion
($\theta$) parameter and fit a _Negative Binomial_ regression to each gene
and alteration pair specified to evaluate the perturbation statistical
significance.
It requires three parameters a UMI count matrix, a list specifying your clones
cell (count matrix columns) composition, and a table indicating the potential
alteration and affected gene to evaluate.

Below we show a quick example of `decal` analysis using a simulated dataset
included with our package.

```{r quick_start}
library(decal)

data("sim_decal")
perturbations <- sim_decal$perturbations
count <- sim_decal$count
clone <- sim_decal$clone

head(perturbations)
```

Our dataset (`sim_decal`) is composed by the three required parameters:

- a UMI count matrix (`count`);
- a list dividing our cells by clonal population (`clone`);
- and a table (`perturbations`) indicating potential interactions to evaluate,
  specified by the perturbed `clone` and candidate `gene`.

With these inputs, we can run the full analysis using the `decal` function as
follow:

```{r quick_start_run}
res <- decal(perturbations, count, clone)
head(res)
```

`decal` function conducts the whole differential analysis, by estimating each
gene dispersion and fitting a negative binomial regression to evaluate each
interaction statistical significance.
Finally, `decal` updates `perturbations` table to include the following
results:

- `n0` and `n1`: number of non-perturbed and perturbed cells, respectively.
- `x0` and `x1`: average UMI count among perturbed and non-perturbed cells,
  respectively.
- `mu`: average UMI count among all cells.
- `xb`: expected average UMI count of perturbed cells.
- `theta`: estimated _negative binomial_ dispersion parameter.
- `z`: estimated perturbation z-score.
- `lfc`: _log2 fold-change_ of perturbed cells gene expression.
- `pvalue` and `p_adjusted`: perturbation _t-test_ significance values.

## Exploring decal results

Let's explore `decal` results. Admitting a _false discovery rate_ (_FDR_) of
5%, we can determinate significantly differentiate interactions by picking
those with `p_adjusted < 0.05`.
Below we present the top 5 interactions with highest reducing and increasing
gene expression effect (as measured by `lfc`).

```{r quick_start_significant}
sig <- subset(res, p_adjusted < 0.05)
sig <- sig[order(sig$lfc), ]
## select first 5 and last 5 as `sig` is ordered by lfc
pick <- c(1:5, nrow(sig) - 1:5)
sig[pick,]
```

To visualize the perturbation effect, let's plot the top 3 interactions
increasing and reducing gene expression.

```{r quick_start_plot_it, fig.height = 4}
## log-transform
log10p <- function(x) log10(x + 1)
## normalizing size factor to average cell depth
sf <- mean(colSums(count)) / colSums(count)
## select first and last 3 rows
pick <- c(1:3, nrow(sig) - 1:3)

par(mfrow = c(2, 3))
for (i in pick) {
  ## Determinate perturbed and unperturbed cells
  x <- colnames(count) %in% clone[[sig$clone[i]]]
  ## Calculate normalized expression count
  y <- count[sig$gene[i],] * sf
  ## plot jitter
  title <- paste0(sig$clone[i], "+", sig$gene[i],"\nLFC: ", round(sig$lfc[i], 2))
  plot(
    x + runif(length(x), -.3, .3), log10p(y), main = title,
    axes = FALSE, xlab = "", ylab = "Normalize UMI count",
    xlim = c(-.5, 1.5), ylim = c(0.9, 1.1) * range(log10p(y))
  )
  axis(1, at = 0:1, labels = c("perturbed", "unperturbed"), las = 2)
  axis(2, at = log10p(c(0, 2, 5, 10, 20, 50, 100)), labels = c(0, 2, 5, 10, 20, 50, 100))
  ## add 95% error bar
  ymed <- c(sig$mu[i], sig$xb[i])
  ylow <- qnbinom(0.025, size = sig$theta[i], mu = ymed)
  yupr <- qnbinom(0.975, size = sig$theta[i], mu = ymed)
  points(c(0, 1), log10p(ymed), col = "firebrick", cex = 2, pch = 16)
  arrows(
    x0 = c(0, 1), y0 = log10p(ylow), x1 = c(0, 1), y1 = log10p(yupr),
    code = 3, angle = 90, col = "firebrick", length = .1, lwd = 2
  )
}
```

## Evaluating `decal` results

Since `sim_decal` is a simulated dataset with some real expression perturbation
introduced, we can measure how well `decal` identifies real change.
The real expression perturbation introduced in our simulated dataset is
indicated by `expected_lfc` column in `perturbations` table, where a
`expected_lfc == 0` indicate no gene was applied.
Admitting a _FDR_ of 5%, we can measure our results with the confusion matrix
below.

```{r quick_start_confusion}
conf_mat <- table(
  real = res$expected_lfc != 0,
  estimated = res$p_adjusted < 0.05
)
round(prop.table(conf_mat, 1), 3)
```

Based on this confusion matrix, our model presented:

- **Accuracy**: `r round(sum(diag(conf_mat)) / sum(conf_mat), 3)`
- **Precision**: `r round(conf_mat[2, 2] / sum(conf_mat[2,]), 3)`
- **Recall**: `r round(conf_mat[2, 2] / sum(conf_mat[,2]), 3)`
- **Observed FDR**: `r round((conf_mat[1, 2] + conf_mat[2, 1]) / sum(conf_mat), 3)`

All of these are good metrics and within the expected results. Next, we can
evaluated how well our model estimated the real _log-2 fold-change_ applied.
Illustrated below is the `lfc` distribution as estimated by `decal` for each
real perturbation value applied (`expected_lfc`).

```{r quick_start_hist}
estimated <- split(res$lfc, res$expected_lfc)

hist_breaks <- c(-Inf, seq(-4, 4, length.out = 50), Inf)
hist_counts <- lapply(estimated, hist, breaks = hist_breaks, plot = FALSE)
hist_ymax <- max(sapply(hist_counts, function(x) x$count / sum(x$count)))

plot(c(-4, 4), c(0, hist_ymax), type = "n",
  xlab = "Estimated LFC", ylab = "Proportion of interactions")
for(i in seq_along(hist_counts)) {
  lines(
    hist_counts[[i]]$mids, hist_counts[[i]]$count / sum(hist_counts[[i]]$count),
    col = i, type = "s"
  )
}
legend("topleft", title = "Real LFC", legend = names(hist_counts),
  col = seq_along(hist_counts), bty="n", lty=1, lwd=2, ncol=2)
```

We can also measure the estimate _Mean Squared Error_ (_MSE_) and _Rooted MSE_
(_RMSE_).

```{r quick_start_rmse}
err <- (res$lfc - res$expected_lfc)**2
err <- split(err, res$expected_lfc)
mse <- sapply(err, mean, na.rm = TRUE)

data.frame(
  N = sapply(err, length),
  MSE = mse,
  RMSE = sqrt(mse)
)
```

Although _MSE_ and _RMSE_ increase for negative _LFC_, the model still presents
a high accuracy and precision when estimating real perturbations effects.

## Understanding arguments

To run `decal` function, it requires three main parameters:

1. `perturbations`, a table of potential interactions to be evaluated,
identified by a `clone` and `gene` pair.
2. `count`, a UMI count matrix.
3. `clone`, a list of cells defining each clone compositions.

### Electing potential gene perturbation

Since `decal` was designed to explore specific gene perturbations, it requires
you to specify clone and gene pairs to evaluate expression change among the
cells within the clone.
In our original study, we located our perturbations and evaluated their impact
to all genes TSS within _250Kbp_ of the perturbation.


Since we aim to explore specific gene perturbations, we must specify pairs of
clone population and genes (or features) to evaluate expression change among
the clone cells in regards to all others.
In our original study, we located our perturbations and evaluated expression
change to all genes with TSS within _250Kbp_ of the perturbation.
Here, we specify the these pairs of clone and perturbed genes to evaluate as
a table with a `clone` and `gene` column indicating the clone/gene id
(as character) or index (as integer), such as the example below:

```{r var_interactions}
head(perturbations)
```

### Defining your experiment clonal structure

`decal` allow you to explore multiple cell groups and explore specific gene
expression change caused to this group of cells. In our experiment, we used
an adjacent dataset to define your experiment clonal populations. This clonal
structure is specified in `clone` as a list of vectors specifying the set of
cells that define each clone. Either as a character vector of cells id
(specify to `count` column names) or a integer vector (specifying `count`
column indexes), such as the example below:

```{r var_clone}
clone[1:2]
```

## Reducing noise and increasing power

To improve results, `decal` include some filtering step to skip testing
interactions with low statistical power.
Among these filter parameters are included:

- `min_x`: minimal average count observed on non-perturbed or perturbed cells
  (indicated by `x0` and `x1`, respectively, on the output table).
- `min_n`: minimal number of perturbed cells (indicated by `n1`).
- `min_mu`: minimal global average count (indicated by `mu`), also
  required to estimate dispersion`theta`.

Interactions that doesn't met all of theses requirements are skipped and no
differential analysis is conducted, though they are still included in the
result table.
By default, `decal` is set to our suggestion that presented a good
performance in our dataset: `min_x = 1`; `min_n = 2`; and `min_mu = 0.05`.

# Understanding decal model

## Statistical model

The `decal` model is based on the observations of _Svensson (2020)_ and
_Townes et al. 2019_ that the UMI counts of a particular gene approximates a
_Poisson_ or _Negative Binomial_ distribution in a single-cell RNA-seq
experiment. Thus, we proposed the observed count with following model:

$$Y_{gc} \sim NB(\mu_{gci}, \theta_g)$$
$$log(\mu_{gci}) = \beta_g + \beta_d D_c + \beta_x X_{ci}$$

where UMI count $Y_{cg}$ for gene _g_ and cell _c_ are modeled using a
_negative binomial_ distribution with fitted mean $\mu_{cgi}$ and a
gene-specific dispersion parameter $\theta_g$. The log fitted mean is
proportional to the cell log total count ($D_c = \sum_g Y_{cg}$) and the
perturbation effect ($\beta_x$) of a specific clone _i_ with $X_{ci}$ is
an indicator variable if the cell belong or not the perturbed clone.

The parameter $\theta_g$ determines the model variance where smaller values
indicate a wider distribution and higher values produce a tighter distribution
that coincides with a Poisson distribution.

$$Var(Y_{cg}) = E[(Y_{cg} - \mu_{cgi})^2] = \mu_{cgi} + \mu_{cgi}^2/\theta_g$$

$$\lim_{\theta\to\infty} NB(\mu, \theta) = Pois(\mu)$$

Our model is similar to previously published tools such as`edgeR`
(_Robinson et al. 2010_; _McCarthy et al. 2012_), `DESeq`
(_Anders and Huber, 2010_; _Love et al. 2014_), and glmGamPoi
(_Ahlmann-Eltze & Huber 2020_).
But it differs in two points: (i) we adopted a regularized strategy to estimate
$\theta_g$ to make it more robust to sampling noise such as described by
_Hafemeister & Satija (2019)_; and (ii)

- TODO: complete section

## Estimating dispersion

Here we apply the strategy used by _Hafemeister & Satija (2019)_ to estimate
$\theta_g$. First, we sample ~ 2000 genes and make a naive estimate of their
expression using a Poisson regression modeled as $Y_{cg} \sim Poi(mu_{cg})$
such as $log(mu_{cg}) = \beta_g + \beta_d D_c$. Next, we estimate a naive
dispersion parameter using a maximum likelihood estimator, fit a kernel smooth
regression to regularize the parameter as a function of average gene count
($\sum_g Y_{cg} / N$), and using this regression to produce the final $\theta_g$
for all other genes.

```{r}
raw_theta <- attr(res, "raw_theta")
mu <- rowMeans(count)
mu_breaks <- c(0.01, 0.1, 1, 2, 5, 10, 20, 50, 100)

plot(log10(mu), raw_theta,
  xlab = "Average expression (mu)",
  ylab = expression(theta), ylim = c(0, 150))
abline(h = 100)
lines(log10(res$mu[order(res$mu)]), res$theta[order(res$mu)], col = 2, lwd = 2)
axis(1, at = log10(mu_breaks), labels = mu_breaks)
legend(
  "topleft", c("Naive Estimate", "Regularized Estimate", "Real"),
  pch = c(1, NA, NA), lty = c(NA, 1, 1), col = c(1, 2, 1)
)
```

# Simulating
xx

Many factor can affect our model **power** and **sensitivity**, thus we also
include tools to simulate and measure your experiments power under your
conditions

Thus we can measure our false discovery rate, given our current cutoff
(`qvalue < 0.10`), by producing the following confusion matrix.

```{r quick_start_confusion_matrix}
# round(prop.table(
#   table(real = res$estimated_lfc != 0, estimated = res$p_adjusted < 0.05)), 2)
```

To power your analysis, I've included some functions that facilitate and allow
you to perform a randomization experiment to estimate the test `z-score` under
no effect and estimate test power under our model assumptions.

## Null hypothesis estimation

```{r sim_randomization, fig.height=3, fig.width=9}
rnd_perturbations <- data.frame(
  clone = sample(names(clone), 1000, replace = TRUE),
  gene = sample(rownames(count), 1000, replace = TRUE)
)
rnd_res <- decal(rnd_perturbations, count, clone)
rnd_res <- subset(rnd_res, !is.na(rnd_res$pvalue))

par(mfrow = c(1, 4))
hist(rnd_res$z, xlab = "Z-score", main = "")
hist(rnd_res$pvalue, xlab = "Observed Pvalue", main = "")
plot(-log10(ppoints(rnd_res$pvalue)), -log10(sort(rnd_res$pvalue)),
  xlab = "Expected -log10(P)",
  ylab = "Observed -log10(P)")
abline(0, 1, col = "red")
hist(rnd_res$p_adjusted, xlab = "Observed Qvalue", main = "")
```

## Power analysis

```{r sim_power, fig.height=4, fig.width=8}
## for each clone perturb 10 genes with -2, -1, 1, and 2 log2 FC and add 100
## interactions with no effect.
lfc <- c(-2, -1, 1, 2)
pwr_lfc <- c(rep(lfc, each = 10), rep(0, 100))
pwr_dat <- sim_experiment_from_data(count, pwr_lfc, nclones = 20, min_n = 5, max_n = 20)
dimnames(pwr_dat$count) <- NULL
pwr_res <- decal(pwr_dat$perturbations, pwr_dat$count, pwr_dat$clone)
pwr_res <- subset(pwr_res, !is.na(pvalue))

## Confusion matrix
prop.table(table(
  actual = pwr_res$expected_lfc != 0,
  estimated = pwr_res$p_adjusted < 0.10
  ),2)

## Histogram of estimated LFC for different effects
breaks <- c(-Inf, seq(-3.5, 3.5, length.out = 30) ,Inf)
histograms <- lapply(
  split(
    pwr_res$lfc[pwr_res$expected_lfc %in% lfc],
    pwr_res$expected_lfc[pwr_res$expected_lfc %in% lfc]
  ), hist, breaks = breaks, plot = FALSE
)
ymax <- max(sapply(histograms, function(x) x$count / sum(x$count))) * 1.05
## Estimate power by LFC
pwr <- sapply(split(
  pwr_res$p_adjusted[pwr_res$expected_lfc %in% lfc] < 0.05,
  pwr_res$expected_lfc[pwr_res$expected_lfc %in% lfc]
  ), mean)

par(mfrow = c(1, 2))
plot(c(-3.5, 3.5), c(0, ymax), type = "n", xlab = "Estimated LFC", ylab = "Frequency")
for(i in seq_along(histograms)) {
  lines(histograms[[i]]$mids, histograms[[i]]$count / sum(histograms[[i]]$count), col = i, type = "s")
}
legend(
  "topleft", title = "Actual LFC", legend = sprintf("%+d", lfc),
  col = seq_along(lfc), bty = "n", lty = 1, cex = .7
)
## Barplot of power estimate
barplot(pwr, xlab = "LFC", ylab = "Power")
```

# Assumptions and Caveats

xxx

# Features

- **tidyverse** compatible

- [@hafemeister_normalization_2019]
- [@svensson_droplet_2020]
- [@townes_feature_2019]
- [@anders_differential_2010]
- [@love_moderated_2014]
- [@ahlmann-eltze_glmgampoi_2021]
- [-@mccarthy_differential_2012]
- [@robinson_edger_2010]

# Session info

```{r sessionInfo}
sessionInfo()
```

# References

<div id="refs"></div>

# Appendix